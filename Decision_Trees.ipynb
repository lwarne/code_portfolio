{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Here I am and make a Regression decision tree class with will work on features with binary values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "\n",
    "In the general regression case for each region $R$ we seek the splitting variable j and splitting point s that solve (ESL 9.13), and partitions the space into two regions $R_1, R_2$ with the lowest total squared error. \n",
    "\n",
    "$$ \\min_{j,s} \\left[ \n",
    "\\min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + \n",
    "\\min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i-c_2)^2 \n",
    "\\right]$$\n",
    "\n",
    "Where $c_1, c_2$ are the average $y_i$ in the child regions. \n",
    "\n",
    "In the classification case minimize use weighted node impurity, Gini or cross entropy, to choose the split.\n",
    "\n",
    "For the regression case with binary features, there is only one possible split point $s$ for each feature, simplifying the search to:\n",
    "\n",
    "$$ \\min_{j} \\left[ \n",
    "\\min_{c_1} \\sum_{x_i \\in R_1(j)} (y_i-c_1)^2 + \n",
    "\\min_{c_2} \\sum_{x_i \\in R_2(j)} (y_i-c_2)^2 \n",
    "\\right]$$\n",
    "\n",
    "The *search* function iterates over all current nodes and will choose the split that have the biggest improvement in the squared error\n",
    "\n",
    "$$ I = \\left\\vert err(R_1) + err(R_2) - err(R) \\right\\vert$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "In mathematical notation, a decicion tree is defined by the splitting variables and points. This is one view of the tree, and if augmented with averages (or modes in classification) at the terminal nodes it is sufficent for prediction. \n",
    "\n",
    "The main question for the above data structure is \"How does an observation fall down the tree\". This is not all that easy. I'm thinking of a dictionary keyed on integers of node number. The value it stores is another dictionary with members below. \n",
    "\n",
    "There will be parent nodes that have splits, and there will be terminal nodes that have predictions. the new format will be\n",
    "\n",
    "- terminal : bool\n",
    "- prediction : float\n",
    "- left : int\n",
    "- right : int\n",
    "- split_feature : int\n",
    "\n",
    "However, during training I need to store membership of the training observations in each node so I can calcualte loss or improvement when greedily seeking splits. \n",
    "\n",
    "This could be done by appending a column to the dataframe or numpy array and storing the integer node membership there. Filtering will then provide access to the observations of the node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Paramaters\n",
    "\n",
    "There is only one tuning paramater for the tree and it determins when we stop growing. One flavor is the number of observations in each node before stopping, another is the complexity paramater at which we stop splitting. \n",
    "\n",
    "Alternativly we can grow a large tree and then prune it down with cross validation.\n",
    "\n",
    "After getting the basics done (fitting a tree to m in each terminal node I will come back to this idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "I'd like to start with a simple model with generated data\n",
    "\n",
    "$$y = \\beta_1 z_1 + \\beta_2 z_2 + \\beta_3 (z_1 \\cdot z_2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, Generic, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta [0.98627683 0.87339195 0.50974552]\n"
     ]
    }
   ],
   "source": [
    "##Generate Data\n",
    "\n",
    "#set seed\n",
    "np.random.seed(2020)\n",
    "\n",
    "#generate data\n",
    "n = 100\n",
    "beta = np.random.uniform(0,1,3)\n",
    "print(\"beta\",beta)\n",
    "X = np.random.randint(0,2,(n,2))\n",
    "y = X[:,0]* beta[0] + X[:,1] * beta[1] + X[:,0] * X[:,1] * beta[2]\n",
    "\n",
    "#check dimensions\n",
    "assert(len(y) == X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up typing\n",
    "T = TypeVar('T')\n",
    "SplitDict = Dict[ str, T ]\n",
    "NodeDict = Dict[ int, SplitDict ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree:\n",
    "    \n",
    "    m : int # minimum of obs in terminal nodes\n",
    "    theta : NodeDict = dict() #history of node splits in tree\n",
    "    \n",
    "    def __init__(self, m = 5):\n",
    "        self.m = m;\n",
    "    \n",
    "    def loss(self, y, ybar):\n",
    "        \"\"\" squared error loss\n",
    "        \"\"\"\n",
    "        return np.sum( np.power(y-ybar,2) )\n",
    "    \n",
    "    def node_split(self,X : np.ndarray ,y : np.ndarray) -> Tuple[ int, float, bool ]:\n",
    "        \"\"\" given a set of observations, finds the feature which produces the minimum split\n",
    "            respects the minimum observation terminal node restriction, will not split \n",
    "            returns:\n",
    "                int : index of feature with lowest total loss split\n",
    "                float : total loss after splitting \n",
    "                bool: If these is any valid split given \n",
    "        \"\"\"\n",
    "        \n",
    "        #check dimensions\n",
    "        assert( len(y) == X.shape[0])\n",
    "        \n",
    "        #iterate over all features and try every split:\n",
    "        min_loss = None\n",
    "        j_split = None\n",
    "        valid = False\n",
    "        for f in range(X.shape[1]): #for each feature\n",
    "            #region 1\n",
    "            y1 = y[X[:,f] == 0]\n",
    "            y1bar = np.mean(y1)\n",
    "            l1 = self.loss(y1,y1bar)\n",
    "            r1size = len(y[X[:,f] == 0])\n",
    "            #region 2\n",
    "            y2 = y[X[:,f] == 1]\n",
    "            y2bar = np.mean(y2)\n",
    "            l2 = self.loss(y2,y2bar)\n",
    "            r2size = len(y[X[:,f] == 0])\n",
    "            \n",
    "            #check if loss decreases, and that does not split beyond min size\n",
    "            if ((min_loss is None) or (l1+l2 < min_loss)) and (r2size >= self.m and r1size >= self.m):\n",
    "                min_loss = l1+l2\n",
    "                j_split = f\n",
    "                valid = True\n",
    "            \n",
    "            print(\"feature {}, total : {:.2f}, left : {:.2f}, right : {:.2f}\".format(f,l1+l2, l1,l2 ))\n",
    "        \n",
    "        return (j_split, min_loss, valid)\n",
    "    \n",
    "    def search(self, \n",
    "               X : np.ndarray,\n",
    "               y : np.ndarray,\n",
    "               mem : np.ndarray\n",
    "              ) -> Tuple[ int, int, bool]:\n",
    "        \"\"\" Iterates over all nodes as defined by mem the node membership array \n",
    "            find the node with the largest improvement I\n",
    "            returns:\n",
    "                int : index of node to split\n",
    "                int : index of feature to split\n",
    "                bool : If there is any valid input \n",
    "        \"\"\"\n",
    "        \n",
    "        #set up variables\n",
    "        max_improvement = None\n",
    "        node_to_split = None\n",
    "        index_of_feature = None\n",
    "        valid = False\n",
    "        \n",
    "        #loop through current terminal nodes\n",
    "        for node in set(mem):\n",
    "            \n",
    "            #get best split for node\n",
    "            split_feature, split_loss, split_valid = self.node_split( X[mem == node], y[mem == node] )\n",
    "            if (split_valid == True):\n",
    "                loss_before = self.loss( y[mem == node], np.mean(y[mem == node]) )\n",
    "                improvement  = np.abs( loss_before - split_loss )\n",
    "            \n",
    "            #check if better and store\n",
    "            if ((split_valid == True) and (max_improvement == None or improvement > max_improvement) ):\n",
    "                valid = split_valid\n",
    "                max_improvement = improvement\n",
    "                node_to_split = node\n",
    "                index_of_feature = split_feature\n",
    "            \n",
    "            #check if there is any actual improvement\n",
    "            if np.isclose(max_improvement,0):\n",
    "                valid = False\n",
    "        \n",
    "        return [ node_to_split, index_of_feature, valid  ]  \n",
    "        \n",
    "\n",
    "    def train(self, \n",
    "              X : np.ndarray,\n",
    "              y : np.ndarray\n",
    "             ) -> None:\n",
    "        \n",
    "        #generate node membership vector\n",
    "        assert(len(y) == X.shape[0])\n",
    "        n, p = X.shape\n",
    "        mem = np.ones(n)\n",
    "        \n",
    "        #loop until fully grown\n",
    "        growing = True\n",
    "        split_count = 0\n",
    "        max_splits = 10\n",
    "        while growing:\n",
    "            \n",
    "            #get best split for current terminal nodes\n",
    "            node_to_split, index_of_feature, valid = self.search(X,y,mem)\n",
    "            \n",
    "            #exit if no valid split\n",
    "            if not valid:\n",
    "                growing = False\n",
    "                break\n",
    "            \n",
    "            #get obeservation index to update\n",
    "            row_index = np.argwhere(mem == node_to_split)\n",
    "            \n",
    "            #for each observation choose the next node\n",
    "            #0 goes left, 1 goes right\n",
    "            next_node = int(np.max(mem) + 1)\n",
    "            left = row_index[ X[row_index,index_of_feature] == 0 ]\n",
    "            right = row_index[ X[row_index,index_of_feature] == 1 ]\n",
    "            \n",
    "            #update\n",
    "            mem[left] = next_node\n",
    "            mem[right] = next_node + 1\n",
    "            \n",
    "          \n",
    "            #add intermediate node to the dictionary with splitting information\n",
    "            #could add prediction if desired\n",
    "            self.theta[node_to_split] = {'terminal' : False,\n",
    "                                    'prediction' : None,\n",
    "                                    'left' : next_node,\n",
    "                                    'right' : next_node + 1,\n",
    "                                    'split_feature' : index_of_feature                \n",
    "                                    }\n",
    "            \n",
    "            \n",
    "            split_count+=1\n",
    "            if split_count >= max_splits:\n",
    "                growing = False\n",
    "            print(\"itr {} done\".format(split_count))\n",
    "            #print(mem)\n",
    "        \n",
    "        #add terminal nodes to dictionary with predictions\n",
    "        for terminal_node in set(mem):\n",
    "            self.theta[terminal_node] = {'terminal' : True,\n",
    "                                        'prediction' : np.mean( y[ np.argwhere(mem == terminal_node) ] ),\n",
    "                                        'left' : None,\n",
    "                                        'right' : None,\n",
    "                                        'split_feature' : None                \n",
    "                                        }\n",
    "            #print(self.theta[terminal_node])\n",
    "    \n",
    "    def get_prediction(self, x : np.ndarray) -> float:\n",
    "        \"\"\" sends a single observation down the regression tree, returns prediction\n",
    "        \"\"\"\n",
    "        next_node = 1\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            #get split info\n",
    "            info = self.theta[next_node]\n",
    "            #return pred if at terminal node\n",
    "            if info['terminal'] == True:\n",
    "                return info['prediction']\n",
    "            #otherwise find which way observation goes\n",
    "            if x[ info[\"split_feature\"]] == 0 :\n",
    "                next_node = info['left']\n",
    "            else:\n",
    "                next_node = info['right']\n",
    "        \n",
    "    \n",
    "    def predict(self, X : np.ndarray) -> np.ndarray:\n",
    "        \"\"\" use the trained tree to predict for observations in X\n",
    "            no checking for correct dimensions as trained\n",
    "            return:\n",
    "                np.ndarray: predicted values for observations\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.array([ self.get_prediction(row) for row in X ])\n",
    "        \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step:\n",
    "- store something as growing that can be used to predidct\n",
    "- predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0, total : 34.44, left : 8.76, right : 25.68\n",
      "feature 1, total : 39.57, left : 12.17, right : 27.41\n",
      "itr 1 done\n",
      "feature 0, total : 8.76, left : 8.76, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 0, total : 25.68, left : 0.00, right : 25.68\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "itr 2 done\n",
      "feature 0, total : 8.76, left : 8.76, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 0, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 0, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "itr 3 done\n",
      "feature 0, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 0, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 0, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 0, total : 0.00, left : 0.00, right : 0.00\n",
      "feature 1, total : 0.00, left : 0.00, right : 0.00\n"
     ]
    }
   ],
   "source": [
    "dt = Decision_Tree()\n",
    "dt.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set loss 1.4298103907130839e-30\n"
     ]
    }
   ],
   "source": [
    "pred = dt.predict(X)\n",
    "loss = dt.loss(pred,y)\n",
    "print(\"training set loss\",loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
